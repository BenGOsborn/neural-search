{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3d800da",
   "metadata": {},
   "source": [
    "# Neural Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7722ce32",
   "metadata": {},
   "source": [
    "An AI powered search engine using Transformers, K-Means, and Cosine Similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a1c053",
   "metadata": {},
   "source": [
    "## Search operation\n",
    "\n",
    "### Preprocessing\n",
    "- Take the text from pandas\n",
    "- Run the model on all of the text\n",
    "- Perform KMeansClustering and record the cluster for each vector\n",
    "\n",
    "### Perform search\n",
    "- Create embeddings\n",
    "- Find all text with the same cluster\n",
    "- Perform cosine similarity search between all elements in the cluster\n",
    "- Return the top K most elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea7f404d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p38/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3708c17",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42466b66",
   "metadata": {},
   "source": [
    "Load CSV file from S3 bucket into DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d405dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cd248d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "\n",
    "bucket_name = sess.default_bucket()\n",
    "role_name = sagemaker.get_execution_role()\n",
    "\n",
    "bucket_prefix = \"neural-search\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e5e59499",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3524: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>overview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Toy Story</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jumanji</td>\n",
       "      <td>When siblings Judy and Peter discover an encha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       title                                           overview\n",
       "0  Toy Story  Led by Woody, Andy's toys live happily in his ...\n",
       "1    Jumanji  When siblings Judy and Peter discover an encha..."
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_LIMIT = 96\n",
    "\n",
    "data_location = f\"s3://{bucket_name}/{bucket_prefix}/data/movies_metadata.csv\"\n",
    "\n",
    "df = pd.read_csv(data_location)[[\"title\", \"overview\"]]\n",
    "df = df[df[\"overview\"].notna()]\n",
    "\n",
    "df = df.head(DATA_LIMIT)\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0df4328",
   "metadata": {},
   "source": [
    "## Load embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894a21eb",
   "metadata": {},
   "source": [
    "Load the sentence embedding transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb5d2c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c1a4b015",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "text_model = BertModel.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a10a00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state\n",
    "    input_mask_expanded = (\n",
    "        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    )\n",
    "    \n",
    "    pooled = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    \n",
    "    return torch.nn.functional.normalize(pooled, p=2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7cf7d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text, tokenizer, model, device):\n",
    "    encoded_input = tokenizer(\n",
    "        text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    \n",
    "    return mean_pooling(model_output, encoded_input[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef893f3b",
   "metadata": {},
   "source": [
    "### Example embedding similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3fb95d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Andy is going to the beach on Sunday with his friends', 'Mandy is going to the movies Tuesday with her Mum', similarity=0.8401806950569153\n",
      "'Andy is going to the beach on Sunday with his friends', 'The cargo ship sailed through the night', similarity=0.561195433139801\n",
      "'Mandy is going to the movies Tuesday with her Mum', 'The cargo ship sailed through the night', similarity=0.5241331458091736\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "sample_text = [\n",
    "    \"Andy is going to the beach on Sunday with his friends\",\n",
    "    \"Mandy is going to the movies Tuesday with her Mum\",\n",
    "    \"The cargo ship sailed through the night\",\n",
    "]\n",
    "\n",
    "sample_embeddings = create_embeddings(sample_text, tokenizer, text_model, device).cpu().detach()\n",
    "\n",
    "tuples = combinations(list(range(sample_embeddings.shape[0])), 2)\n",
    "\n",
    "for i, j in tuples:\n",
    "    similarity = cosine_similarity_model(sample_embeddings[i], sample_embeddings[j])\n",
    "    print(f\"'{sample_text[i]}', '{sample_text[j]}', similarity={similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f15dc30",
   "metadata": {},
   "source": [
    "## Create embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1b1e2e",
   "metadata": {},
   "source": [
    "Create the embeddings representing each movie description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "318bd59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02961762,  0.0277533 ,  0.02578685, ...,  0.02357359,\n",
       "         0.01307014, -0.00316987],\n",
       "       [-0.02399799,  0.0163674 ,  0.03241539, ...,  0.0048577 ,\n",
       "         0.00903032,  0.02823391],\n",
       "       [ 0.00591561, -0.02498905,  0.03475502, ..., -0.01966395,\n",
       "         0.00797963, -0.03016624],\n",
       "       ...,\n",
       "       [-0.01262194,  0.04083531,  0.04546607, ..., -0.00758757,\n",
       "         0.01327129, -0.01018843],\n",
       "       [ 0.01855192,  0.03487618,  0.04439139, ..., -0.01809348,\n",
       "         0.03097658, -0.02043471],\n",
       "       [ 0.01045637, -0.00373105,  0.06106191, ..., -0.01639825,\n",
       "         0.0091132 , -0.01901365]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "embeddings = np.zeros((len(input_text), sample_embeddings.shape[-1]))\n",
    "\n",
    "input_text = df[\"overview\"].values\n",
    "\n",
    "for i in range(0, len(input_text), BATCH_SIZE):\n",
    "    batch = list(input_text[i:i + BATCH_SIZE])\n",
    "    \n",
    "    batch_embeddings = create_embeddings(batch, tokenizer, text_model, device).cpu().detach().numpy()\n",
    "    embeddings[i:i + BATCH_SIZE, :] = batch_embeddings\n",
    "    \n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3afdc02",
   "metadata": {},
   "source": [
    "## Train K-Means model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b8ece3",
   "metadata": {},
   "source": [
    "Train model used for clustering embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "03c5d69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "38f3fdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_model = KMeans().fit(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b3d52d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>overview</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Toy Story</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jumanji</td>\n",
       "      <td>When siblings Judy and Peter discover an encha...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       title                                           overview  cluster\n",
       "0  Toy Story  Led by Woody, Andy's toys live happily in his ...        4\n",
       "1    Jumanji  When siblings Judy and Peter discover an encha...        4"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"cluster\"] = kmeans_model.labels_\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18166f9b",
   "metadata": {},
   "source": [
    "## Perform search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a91baa3",
   "metadata": {},
   "source": [
    "Search the database to find a list of movies that match the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "08877877",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_model = torch.nn.CosineSimilarity(dim=0, eps=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "bdda47b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(queries, tokenizer, text_model, kmeans_model, df, embeddings, device, max_results, cosine_similarity_model):\n",
    "    query_embeddings = create_embeddings(queries, tokenizer, text_model, device).cpu().detach().numpy().astype(float)\n",
    "    \n",
    "    clusters = kmeans_model.predict(query_embeddings)\n",
    "    \n",
    "    out = []\n",
    "    \n",
    "    for i in range(len(clusters)):\n",
    "        mask = df[\"cluster\"] == clusters[i]\n",
    "        \n",
    "        temp = []\n",
    "        \n",
    "        for j in mask.keys():\n",
    "            if mask[j]:\n",
    "                similarity = cosine_similarity_model(torch.tensor(query_embeddings[i]), torch.tensor(embeddings[j]))\n",
    "                temp.append((similarity, clusters[i], df[\"title\"][j], df[\"overview\"][j]))\n",
    "        \n",
    "        temp = sorted(temp, reverse=True, key=lambda x: x[0])[:MAX_RESULTS]\n",
    "        out.append(temp)\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "37b389d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(tensor(0.5883, dtype=torch.float64),\n",
       "   7,\n",
       "   'Casino',\n",
       "   'The life of the gambling paradise – Las Vegas – and its dark mafia underbelly.'),\n",
       "  (tensor(0.4831, dtype=torch.float64),\n",
       "   7,\n",
       "   'Richard III',\n",
       "   \"Shakespeare's Play transplanted into a 1930s setting.\"),\n",
       "  (tensor(0.3891, dtype=torch.float64),\n",
       "   7,\n",
       "   \"Things to Do in Denver When You're Dead\",\n",
       "   'A mafia film in Tarantino style with a star-studded cast. Jimmy’s “The Saint” gangster career has finally ended. Yet now he finds him self doing favors for a wise godfather known as “The Man with the Plan.”')],\n",
       " [(tensor(0.4448, dtype=torch.float64),\n",
       "   7,\n",
       "   'Casino',\n",
       "   'The life of the gambling paradise – Las Vegas – and its dark mafia underbelly.'),\n",
       "  (tensor(0.4254, dtype=torch.float64),\n",
       "   7,\n",
       "   \"Things to Do in Denver When You're Dead\",\n",
       "   'A mafia film in Tarantino style with a star-studded cast. Jimmy’s “The Saint” gangster career has finally ended. Yet now he finds him self doing favors for a wise godfather known as “The Man with the Plan.”'),\n",
       "  (tensor(0.3584, dtype=torch.float64),\n",
       "   7,\n",
       "   'Richard III',\n",
       "   \"Shakespeare's Play transplanted into a 1930s setting.\")],\n",
       " [(tensor(0.4750, dtype=torch.float64),\n",
       "   7,\n",
       "   \"Things to Do in Denver When You're Dead\",\n",
       "   'A mafia film in Tarantino style with a star-studded cast. Jimmy’s “The Saint” gangster career has finally ended. Yet now he finds him self doing favors for a wise godfather known as “The Man with the Plan.”'),\n",
       "  (tensor(0.4506, dtype=torch.float64),\n",
       "   7,\n",
       "   'Casino',\n",
       "   'The life of the gambling paradise – Las Vegas – and its dark mafia underbelly.'),\n",
       "  (tensor(0.3948, dtype=torch.float64),\n",
       "   7,\n",
       "   'Richard III',\n",
       "   \"Shakespeare's Play transplanted into a 1930s setting.\")]]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_RESULTS = 5\n",
    "\n",
    "queries = [\n",
    "    \"gambling and mafia and crime\",\n",
    "    \"food is tasty\",\n",
    "    \"magical realm with siblings\"\n",
    "]\n",
    "\n",
    "results = search(queries, tokenizer, text_model, kmeans_model, df, embeddings, device, MAX_RESULTS, cosine_similarity_model)\n",
    "\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
